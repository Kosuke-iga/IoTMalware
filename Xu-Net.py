import time
import os 
import re
import keras
from keras.preprocessing.image import load_img, img_to_array
import numpy as np
import matplotlib.pylab as plt
from matplotlib import cm
import tensorflow as tf
from sklearn.datasets import fetch_mldata

def global_average_pooling(x):
    for _ in range(2):
        x = tf.reduce_mean(x, axis=1)
    return x

np.set_printoptions(threshold=1000)

# ディレクトリ内の各形式の画像をlistとして抽出
def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm|pgm'):
    return [os.path.join(root, f)
            for root, _, files in os.walk(directory) for f in sorted(files)
            if re.match(r'([\w]+\.(?:' + ext + '))', f.lower())]

# Xavier Init
def xavier_init(n_inputs, n_outputs, uniform=True):
  """Set the parameter initialization using the method described.
  This method is designed to keep the scale of the gradients roughly the same
  in all layers.
  Xavier Glorot and Yoshua Bengio (2010):
           Understanding the difficulty of training deep feedforward neural
           networks. International conference on artificial intelligence and
           statistics.
  Args:
    n_inputs: The number of input nodes into each output.
    n_outputs: The number of output nodes for each input.
    uniform: If true use a uniform distribution, otherwise use a normal.
  Returns:
    An initializer.
  """
  if uniform:
    # 6 was used in the paper.
    init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))
    return tf.random_uniform_initializer(-init_range, init_range)
  else:
    # 3 gives us approximately the same limits as above since this repicks
    # values greater than 2 standard deviations from the mean.
    stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))
    return tf.truncated_normal_initializer(stddev=stddev)


#  LOG_DIR_NAME = 'resized_wow_040_twice'
#  LOG_DIR_NAME = 'resized_wow_040_3times'

# データを読み込むディレクトリ
#  TRAIN_COVER_DIR = '../resized_256_image/train/cover/'
#  TRAIN_STEGO_DIR = '../resized_256_image/train/stego_wow040/'
#  TEST_COVER_DIR = '../resized_256_image/valid/cover/'
#  TEST_STEGO_DIR = '../resized_256_image/valid/stego_wow040/'

#  TRAIN_COVER_DIR = '../resized_256_image/train/nearest/stego_wow040/'
#  TRAIN_STEGO_DIR = '../resized_256_image/train/nearest/stego_wow040_twice/'
#  TEST_COVER_DIR = '../resized_256_image/valid/nearest/stego_wow040/'
#  TEST_STEGO_DIR = '../resized_256_image/valid/nearest/stego_wow040_twice/'

#  TRAIN_COVER_DIR = '../resized_256_image/train/wow_5times_020/'
#  TRAIN_STEGO_DIR = '../resized_256_image/train/wow_6times_020/'
#  TEST_COVER_DIR = '../resized_256_image/valid/wow_5times_020/'
#  TEST_STEGO_DIR = '../resized_256_image/valid/wow_6times_020/'

#  TRAIN_COVER_DIR = '../resized_256_image/train/crop/cover/'
#  TRAIN_STEGO_DIR = '../resized_256_image/train/crop/random_key/stego_wow_040/'
#  TEST_COVER_DIR = '../resized_256_image/valid/crop/cover/'
#  TEST_STEGO_DIR = '../resized_256_image/valid/crop/random_key/stego_wow_040/'

#  TRAIN_COVER_DIR = '../cropped_256_quarter_image/train/cover/'
#  TRAIN_STEGO_DIR = '../cropped_256_quarter_image/train/stego_wow040/'
#  TEST_COVER_DIR = '../cropped_256_quarter_image/valid/cover/'
#  TEST_STEGO_DIR = '../cropped_256_quarter_image/valid/stego_wow040/'

#  TRAIN_COVER_DIR = './Steganalysis_dataset/train/cover/'
#  TRAIN_STEGO_DIR = './Steganalysis_dataset/train/040/WOW/wow_040/'
#  TEST_COVER_DIR = './Steganalysis_dataset/valid/cover/'
#  TEST_STEGO_DIR = './Steganalysis_dataset/valid/040/WOW/wow_040/'

#  TRAIN_COVER_DIR = './Steganalysis_dataset/train/cover_crroped_256/'
#  TRAIN_STEGO_DIR = './Steganalysis_dataset/train/cropped_wow_040/'
#  TEST_COVER_DIR = './Steganalysis_dataset/valid/cover_cropped_256/'
#  TEST_STEGO_DIR = './Steganalysis_dataset/valid/cropped_wow_040/'

TRAIN_COVER_DIR = './Steganalysis_dataset/train/nearest/random_key/cover_suni_040_twice_seed_50/'
TRAIN_STEGO_DIR = './Steganalysis_dataset/train/nearest/random_key/stego_suni_040_3times_seed_60/'
TEST_COVER_DIR = './Steganalysis_dataset/valid/nearest/random_key/cover_suni_040_twice_seed_50/'
TEST_STEGO_DIR = './Steganalysis_dataset/valid/nearest/random_key/stego_suni_040_3times_seed_60/'

 # 入力画像の次元とチャンネル
img_rows, img_cols, img_channels = 256, 256, 1
CLASS_NUM = 2 # クラス数（何値分類か）
RESTORE_MODEL = 0  # モデルを復元したいかどうか
RESTORE_ITER_NUM = 10000  # 復元したいモデルのイテレーション
MODEL_SAVE_STEP = 10000  # 何イテレーションでmodelを保存するか
TRAIN_BATCH_SIZE = 5  # バッチサイズ（一回で学習する画像数）
TEST_BATCH_SIZE = 5
EPOCH_NUM = 765 * 2  # エポック数（学習データセット分を一回とした学習を何回行うか）
LEARNING_RATE = 0.00001 # 学習率
print(LEARNING_RATE)
LOG_DIR_PATH = './Xu-Net_logFiles/'
LOG_DIR_PATH = './Xu-Net_logFiles/gomi_log/'
LOG_DIR_NAME = 'cover_wow_040_twice_seed_20_vs_stego_wow_3times_040_seed_10' + str(LEARNING_RATE)
 
SELECT_NUM = 50 # ここでデータセットの数を指定

X_train_cover = [] 
X_test_cover = []
X_train_stego = [] 
X_test_stego = []
Y_train_cover = [] 
Y_test_cover = []
Y_train_stego = [] 
Y_test_stego = []

# カバー画像のピクセルデータをXに，ラベルデータをYに格納
for picture in list_pictures(TRAIN_COVER_DIR):
    #  print(picture)
    img = img_to_array(load_img(picture, grayscale=True))
    X_train_cover.append(img)
    Y_train_cover.append(0)
for picture in list_pictures(TEST_COVER_DIR):
    img = img_to_array(load_img(picture, grayscale=True))
    X_test_cover.append(img)
    Y_test_cover.append(0)
# ステゴ画像のピクセルデータをXに，ラベルデータをYに格納
for picture in list_pictures(TRAIN_STEGO_DIR):
    #  print(picture)
    img = img_to_array(load_img(picture, grayscale=True))
    X_train_stego.append(img)
    Y_train_stego.append(1)
for picture in list_pictures(TEST_STEGO_DIR):
    img = img_to_array(load_img(picture, grayscale=True))
    X_test_stego.append(img)
    Y_test_stego.append(1)

# X, Yをnumpyに変換(Yは行列の形も縦長になるように変換)
X_train_cover = np.array(X_train_cover) 
X_train_stego = np.array(X_train_stego)
X_test_cover = np.array(X_test_cover) 
X_test_stego = np.array(X_test_stego)
Y_train_cover = np.array(Y_train_cover).reshape(len(Y_train_cover), 1)
Y_train_stego = np.array(Y_train_stego).reshape(len(Y_train_stego), 1)
Y_test_cover = np.array(Y_test_cover).reshape(len(Y_test_cover), 1)
Y_test_stego = np.array(Y_test_stego).reshape(len(Y_test_stego), 1)


# 教師データを変換
X_train_cover = X_train_cover.reshape((len(X_train_cover), img_rows, img_cols, 1)) # (N, height, width, channel)
X_train_stego = X_train_stego.reshape((len(X_train_stego), img_rows, img_cols, 1)) # (N, height, width, channel)
X_test_cover = X_test_cover.reshape((len(X_test_cover), img_rows, img_cols, 1)) # (N, height, width, channel)
X_test_stego = X_test_stego.reshape((len(X_test_stego), img_rows, img_cols, 1)) # (N, height, width, channel)
#  print(X_test_cover.shape)
# ラベルはone-hotベクトルに変換する
#  train_y = np.eye(np.max(train_y)+1)[train_y]
#  test_y = np.eye(np.max(test_y)+1)[test_y]
# one_hotに変換
Y_train_cover = keras.utils.to_categorical(Y_train_cover, CLASS_NUM)
Y_test_cover = keras.utils.to_categorical(Y_test_cover, CLASS_NUM)
Y_train_stego = keras.utils.to_categorical(Y_train_stego, CLASS_NUM)
Y_test_stego = keras.utils.to_categorical(Y_test_stego, CLASS_NUM)
#  print(Y_test_cover)

# 数を少なくして試したい場合 ===============
X_train_cover = X_train_cover[0:SELECT_NUM, :, : , :]
X_train_stego = X_train_stego[0:SELECT_NUM, :, : , :]
Y_train_cover = Y_train_cover[0:SELECT_NUM, :]
Y_train_stego = Y_train_stego[0:SELECT_NUM, :]
X_test_cover = X_test_cover[0:SELECT_NUM, :, : , :]
X_test_stego = X_test_stego[0:SELECT_NUM, :, : , :]
Y_test_cover = Y_test_cover[0:SELECT_NUM, :]
Y_test_stego = Y_test_stego[0:SELECT_NUM, :]
# ==========================================
X_test = np.append(X_test_cover, X_test_stego, axis=0)
X_train = np.append(X_train_cover, X_train_stego, axis=0)
Y_test = np.append(Y_test_cover, Y_test_stego, axis=0)
Y_train = np.append(Y_train_cover, Y_train_stego, axis=0)
print(Y_test.shape)
print(X_test.shape)
 
# ネットワークの定義
x_ = tf.placeholder(tf.float32, shape=(None, img_rows, img_cols, 1))
y_ = tf.placeholder(tf.float32, shape=(None, CLASS_NUM))
#  is_training = tf.placeholder_with_default(True,shape=[])
is_training = tf.placeholder_with_default(False,shape=[])
#  is_training = tf.get_variable(False, name='training', trainable=False) 
#  is_training = tf.get_variable('is_training', dtype=tf.bool, \
                              #  initializer=True, trainable=False)
# HPF
with tf.variable_scope('HPF'):
    kv_data = tf.constant\
              ([[[[-1/12.]],[[ 2/12.]], [[-2/12.]], [[2/12.]], [[-1/12.]]],\
                [[[2/12.]],[[-6/12.]], [[8/12.]], [[-6/12.]], [[2/12.]]],\
                [[[-2/12.]],[[8/12.]], [[-12/12.]], [[8/12.]], [[-2/12.]]],\
                [[[2/12.]],[[-6/12.]], [[8/12.]], [[-6/12.]], [[2/12.]]],\
                [[[-1/12.]],[[2/12.]], [[-2/12.]], [[2/12.]], [[-1/12.]]]],\
              dtype = tf.float32)
    kv = tf.reshape(kv_data, shape = [5, 5, 1, 1])
    after_hfp = tf.nn.conv2d(x_, kv, strides=[1, 1, 1, 1], padding="SAME")
# 畳み込み層1
with tf.variable_scope('Group1'):
    avg_pool_size1 = 5 # 畳み込み層1のマックスプーリングサイズ
    fil = tf.get_variable('weights', [5, 5, 1, 8], dtype=tf.float32, 
            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))
    conv1_c2 = tf.nn.conv2d(after_hfp, fil, strides=[1, 1, 1, 1], padding="SAME") # 畳み込み層1-畳み込み
    conv1_abs = tf.abs(conv1_c2) # ABS
    #  conv1_bn = tf.layers.BatchNormalization()(conv1_abs, training=True)  # BatchNormalization
    #  BN = tf.layers.BatchNormalization(momentum=0.9)
    BN = tf.layers.BatchNormalization()
    conv1_bn = tf.cond(is_training,lambda: BN(conv1_abs,training=True),lambda: BN(conv1_abs,training=False))
    conv1_tanh = tf.math.tanh(conv1_bn) # TanH 
    conv1_ap = tf.nn.avg_pool(conv1_tanh, \
            ksize=[1, avg_pool_size1, avg_pool_size1, 1], \
            strides=[1, 2, 2, 1], \
            padding="SAME") # 畳み込み層1-マックスプーリング
# 畳み込み層2
with tf.variable_scope('Group2'):
    avg_pool_size2 = 5 # 畳み込み層2のプーリングサイズ
    fil2 = tf.get_variable('weights', [5, 5, 8, 16], dtype=tf.float32, 
            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))
    conv2_c2 = tf.nn.conv2d(conv1_ap, fil2, strides=[1, 1, 1, 1], padding="SAME") # 畳み込み層1-畳み込み
    #  conv2_bn = tf.layers.BatchNormalization()(conv2_c2, training=True)  # BatchNormalization
    BN2 = tf.layers.BatchNormalization()
    conv2_bn = tf.cond(is_training,lambda: BN2(conv2_c2,training=True),lambda: BN2(conv2_c2,training=False))
    conv2_tanh = tf.math.tanh(conv2_bn) # TanH 
    conv2_ap = tf.nn.avg_pool(conv2_tanh, \
            ksize=[1, avg_pool_size2, avg_pool_size2, 1], \
            strides=[1, 2, 2, 1], \
            padding="SAME") # 畳み込み層2-averageプーリング
# 畳み込み層3
with tf.variable_scope('Group3'):
    avg_pool_size3 = 5 # 畳み込み層3のプーリングサイズ
    fil3 = tf.get_variable('weights', [1, 1, 16, 32], dtype=tf.float32, 
            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))
    conv3_c2 = tf.nn.conv2d(conv2_ap, fil3, strides=[1, 1, 1, 1], padding="SAME") # 畳み込み層1-畳み込み
    #  conv3_bn = tf.layers.BatchNormalization()(conv3_c2, training=True)  # BatchNormalization
    BN3 = tf.layers.BatchNormalization()
    conv3_bn = tf.cond(is_training,lambda: BN3(conv3_c2,training=True),lambda: BN3(conv3_c2,training=False))
    conv3_relu = tf.nn.relu(conv3_bn) # ReLU
    conv3_ap = tf.nn.avg_pool(conv3_relu, \
            ksize=[1, avg_pool_size3, avg_pool_size3, 1], \
            strides=[1, 2, 2, 1], \
            padding="SAME") # 畳み込み層3-averageプーリング
# 畳み込み層4
with tf.variable_scope('Group4'):
    avg_pool_size4 = 5 # 畳み込み層3のプーリングサイズ
    fil4 = tf.get_variable('weights', [1, 1, 32, 64], dtype=tf.float32, 
            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))
    conv4_c2 = tf.nn.conv2d(conv3_ap, fil4, strides=[1, 1, 1, 1], padding="SAME") # 畳み込み層1-畳み込み
    #  conv4_bn = tf.layers.BatchNormalization()(conv4_c2, training=True)  # BatchNormalization
    BN4 = tf.layers.BatchNormalization()
    conv4_bn = tf.cond(is_training,lambda: BN4(conv4_c2,training=True),lambda: BN4(conv4_c2,training=False))
    conv4_relu = tf.nn.relu(conv4_bn) # ReLU
    conv4_ap = tf.nn.avg_pool(conv4_relu, \
            ksize=[1, avg_pool_size4, avg_pool_size4, 1], \
            strides=[1, 2, 2, 1], \
            padding="SAME") # 畳み込み層3-averageプーリング
# 畳み込み層5
with tf.variable_scope('Group5'):
    avg_pool_size5 = 32 # 畳み込み層3のプーリングサイズ
    fil5 = tf.get_variable('weights', [1, 1, 64, 128], dtype=tf.float32, 
            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01))
    conv5_c2 = tf.nn.conv2d(conv4_ap, fil5, strides=[1, 1, 1, 1], padding="SAME") # 畳み込み層1-畳み込み
    #  conv5_bn = tf.layers.BatchNormalization()(conv5_c2, training=True)  # BatchNormalization
    BN5 = tf.layers.BatchNormalization()
    conv5_bn = tf.cond(is_training,lambda: BN5(conv5_c2,training=True),lambda: BN5(conv5_c2,training=False))
    conv5_relu = tf.nn.relu(conv5_bn) # ReLU
    conv5_ap = global_average_pooling(conv5_relu)
# 全結合層1
with tf.variable_scope('Fully_connected'):
    OUTPUT_SIZE = 2
    INPUT_SIZE = 128
    regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)
    W5 = tf.get_variable("W5",
                         shape=[INPUT_SIZE, OUTPUT_SIZE],
                         dtype=tf.float32,
                         regularizer=regularizer,
                         initializer=tf.contrib.layers.xavier_initializer())
                         #  initializer=xavier_init(INPUT_SIZE, OUTPUT_SIZE))
    #  tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, W5)
    b5 = tf.get_variable("b5",
                         shape=[OUTPUT_SIZE],
                         dtype=tf.float32,
                         initializer=tf.contrib.layers.xavier_initializer())
                         #  initializer=xavier_init(INPUT_SIZE, OUTPUT_SIZE))
    fc = tf.reshape(conv5_ap, [-1, INPUT_SIZE])
    y = tf.matmul(fc, W5) + b5
# クロスエントロピー誤差
beta = 0.001 # L2 regularization のパラメータ？
#  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y) + beta * tf.nn.l2_loss(W5))
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, 'Fully_connected')
cross_entropy = cross_entropy + tf.reduce_sum(reg_variables)
# 勾配法
update_step = 5000  # 学習率の更新間隔
decay_rate = 0.9  # 学習率の減衰割合
global_step = tf.Variable(0, trainable=False)
#  Passing global_step to minimize() will increment it at each step.
scheduled_learning_rate = \
        tf.train.exponential_decay(LEARNING_RATE, 
        global_step, update_step, decay_rate, staircase=True)
extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) 
# 以下の3行を書かないとバッチ正規化の移動平均とかを更新してくれないらしい
with tf.control_dependencies(extra_update_ops):  
    optimizer = tf.train.MomentumOptimizer(learning_rate=scheduled_learning_rate, momentum=0.9)
    train_op = optimizer.minimize(cross_entropy, global_step=global_step)

# 正解率の計算
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
gpunum = 0
config = tf.ConfigProto(
    gpu_options=tf.GPUOptions(
    visible_device_list=str(gpunum), # specify GPU number
    allow_growth=True
        )
    ) 
 
# 学習
print("Train")
all_train_num = len(X_train_cover) + len(X_train_stego)
half_train_num = int(all_train_num*0.5) #  cover とstegoを同じ数ずつ入れるので
step_num = int(half_train_num/4) # 一気に学習とテストデータを入れられないので区切る用のパラメータ
write_step = 1000  # TensorBoard にグラフをプロットする間隔（iteration）
iteration_num = 0

with tf.Session(config=config) as sess:
    st = time.time()
    #  print(sess.run(kv_data))
    with tf.name_scope("train") as scope:
        train_acc, train_acc_op = tf.metrics.mean(accuracy, name="train_acc_mean")
        acc_summary_train = tf.summary.scalar("train_acc", train_acc)
        train_loss, train_loss_op = tf.metrics.mean(cross_entropy, name="train_loss_mean")
        loss_summary_train = tf.summary.scalar("train_loss", train_loss) 
    with tf.name_scope("test") as scope:
        test_acc, test_acc_op = tf.metrics.mean(accuracy, name="test_acc_mean")
        acc_summary_test = tf.summary.scalar("test_acc", test_acc)
        test_loss, test_loss_op = tf.metrics.mean(cross_entropy, name="test_loss_mean")
        loss_summary_test = tf.summary.scalar("test_loss", test_loss)
    # metrics.meanをリセットできるようにする
    reset_vars = [i for i in tf.local_variables() if i.name.split('/')[0] in ['train', 'test']]
    #  print(reset_vars)
    reset_op = [tf.variables_initializer(reset_vars)]
    writer = tf.summary.FileWriter(LOG_DIR_PATH + LOG_DIR_NAME + '/', graph_def=sess.graph_def)
    saver = tf.train.Saver()
    if RESTORE_MODEL: # 　モデルを途中からやる場合
        model_checkpoint_path = LOG_DIR_PATH + LOG_DIR_NAME + \
                '/' + str(RESTORE_ITER_NUM) + "_model.ckpt"
        #  last_model = ckpt.model_checkpoint_path # 最後に保存したmodelへのパス
        #  print("load " + last_model)
        saver.restore(sess, model_checkpoint_path) # 変数データの読み込み
        iteration_num = RESTORE_ITER_NUM
    else: # 保存データがない場合
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer()) # tf.metrics.meanの計算に必要
    for epoch in range(EPOCH_NUM):
        sff_idx = np.random.permutation(half_train_num)
        #  print(sff_idx)
        for i in range(0, half_train_num, int(TRAIN_BATCH_SIZE*0.5)):
            # Train_batch_x ===============
            # indexの大きさ超えちゃったら終端までのif文も追加
            batch_x = [] 
            print('cover', X_train_cover[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            print('stego', X_train_stego[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            batch_x.extend(X_train_cover[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            batch_x.extend(X_train_stego[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            batch_x = np.array(batch_x)
            # ===========================
            # Test_batch_x ===============
            #  test_batch_x = [] 
            #  test_batch_x.extend(X_test_cover[sff_idx[i:i+int(TEST_BATCH_SIZE*0.5)
                #  if i+int(TEST_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            #  test_batch_x.extend(X_test_stego[sff_idx[i:i+int(TEST_BATCH_SIZE*0.5)
                #  if i+int(TEST_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            #  test_batch_x = np.array(test_batch_x)
            # =========================== 
            # Train_batch_y  ===============
            batch_y = [] 
            batch_y.extend(Y_train_cover[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            batch_y.extend(Y_train_stego[sff_idx[i:i+int(TRAIN_BATCH_SIZE*0.5)
                if i+int(TRAIN_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            batch_y = np.array(batch_y)
            # Test_batch_y ===============
            #  test_batch_y = [] 
            #  test_batch_y.extend(Y_test_cover[sff_idx[i:i+int(TEST_BATCH_SIZE*0.5)
                #  if i+int(TEST_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            #  test_batch_y.extend(Y_test_stego[sff_idx[i:i+int(TEST_BATCH_SIZE*0.5)
                #  if i+int(TEST_BATCH_SIZE*0.5) < half_train_num else half_train_num]])
            #  test_batch_y = np.array(test_batch_y)
            # ===========================
            # 学習
            sess.run(train_op, feed_dict={x_: batch_x, y_: batch_y, is_training:True})
            # 出力 & テスト
            if (iteration_num) % write_step == 0:
                ed = time.time()
                lr = sess.run(scheduled_learning_rate)
                 # Write log to TensorBoard
                train_list = [train_loss_op, train_acc_op]
                # 平均計算をリセット
                sess.run(reset_op)
                # バッチをまたいだ平均の計算
                for i in range(0, half_train_num, step_num):
                    step_train_X = [] 
                    step_train_X.extend(X_train_cover[i:i+step_num])
                    step_train_X.extend(X_train_stego[i:i+step_num])
                    step_train_X = np.array(step_train_X)
                    step_train_Y = [] 
                    step_train_Y.extend(Y_train_cover[i:i+step_num])
                    step_train_Y.extend(Y_train_stego[i:i+step_num])
                    step_train_Y = np.array(step_train_Y)
                    sess.run(train_list, feed_dict={
                        x_: step_train_X, y_: step_train_Y, is_training: False})
                # 平均値取得（損失関数の平均値とそのsummaryを返す）
                train_result_print = sess.run([train_loss, train_acc])
                train_result = sess.run([loss_summary_train, acc_summary_train])
                for j in range(0,len(train_result)):
                    writer.add_summary(train_result[j], iteration_num)
                #  print("Train accuracy at step %s: %s %s" % (iteration_num, result[0], str(ed-st))) 
                test_list = [test_loss_op, test_acc_op]
                #  test_result = sess.run(val_list, feed_dict={x_: test_batch_x, y_:test_batch_y})
                # 平均計算をリセット
                sess.run(reset_op)
                # バッチをまたいだ平均の計算
                for i in range(0, half_train_num, step_num):
                    step_test_X = [] 
                    step_test_X.extend(X_test_cover[i:i+step_num])
                    step_test_X.extend(X_test_stego[i:i+step_num])
                    step_test_X = np.array(step_test_X)
                    step_test_Y = [] 
                    step_test_Y.extend(Y_test_cover[i:i+step_num])
                    step_test_Y.extend(Y_test_stego[i:i+step_num])
                    step_test_Y = np.array(step_test_Y)
                    tl, ta = sess.run(test_list, feed_dict={
                        x_: step_test_X, y_: step_test_Y, is_training: False})
                    #  print('test loss', tl)
                    #  print('test acc', ta)
                    #  print('average loss', sess.run(test_loss))
                #  test_result = sess.run(val_list, feed_dict={x_: selected_test_x, y_:selected_test_y, is_training:False})
                test_result_print = sess.run([test_loss, test_acc])
                test_result = sess.run([loss_summary_test, acc_summary_test])
                #  test_loss = cross_entropy.eval(feed_dict={x_: test_batch_x, y_: test_batch_y})
                for j in range(0,len(test_result)):
                    writer.add_summary(test_result[j], iteration_num) 
                print("Iteration:{} train loss:{} train acc:{} val loss:{} val acc:{} lr:{} time:{}"\
                        .format(iteration_num, train_result_print[0], train_result_print[1], 
                            test_result_print[0], test_result_print[1], lr, ed-st))
                #  print("Validation accuracy at %s iteration: %s %s" % (iteration_num, result[0], str(ed-st)))
                st = time.time() 
            if (iteration_num) % MODEL_SAVE_STEP == 0:
                saver.save(sess, LOG_DIR_PATH + LOG_DIR_NAME + 
                         '/' + str(iteration_num) + "_model.ckpt") # 保存
            iteration_num += 1
